{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL script for 'part_categories' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/part_categories.sql'.\n",
      "SQL script for 'inventories' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/inventories.sql'.\n",
      "SQL script for 'parts' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/parts.sql'.\n",
      "SQL script for 'inventory_sets' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/inventory_sets.sql'.\n",
      "SQL script for 'inventory_parts' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/inventory_parts.sql'.\n",
      "SQL script for 'colors' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/colors.sql'.\n",
      "SQL script for 'sets' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/sets.sql'.\n",
      "SQL script for 'themes' generated and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql/themes.sql'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def generate_sql_from_csv(csv_file, table_name):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Step 1: Generate CREATE TABLE statement\n",
    "    create_table_sql = f'CREATE TABLE \"{table_name}\" (\\n'\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            sql_type = 'bigint'\n",
    "        elif df[col].dtype == 'float64':\n",
    "            sql_type = 'float'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            sql_type = 'datetime'\n",
    "        else:\n",
    "            sql_type = 'text'\n",
    "        create_table_sql += f'  \"{col}\" {sql_type},\\n'\n",
    "    create_table_sql = create_table_sql.rstrip(\",\\n\") + \"\\n);\\n\\n\"\n",
    "    \n",
    "    # Step 2: Generate INSERT INTO statements\n",
    "    column_list = \", \".join(f'\"{col}\"' for col in df.columns)\n",
    "    insert_into_sql = f'INSERT INTO \"{table_name}\" ({column_list})\\nVALUES\\n'\n",
    "    \n",
    "    values = []\n",
    "    for _, row in df.iterrows():  # Correct indentation\n",
    "        row_values = []\n",
    "        for col in df.columns:  # Access each column explicitly\n",
    "            value = row[col]\n",
    "            if pd.isnull(value):  # Handle NULL values\n",
    "                row_values.append(\"NULL\")\n",
    "            elif isinstance(value, str):  # Handle string values\n",
    "                # Escape single quotes properly in SQL by doubling them\n",
    "                escaped_value = value.replace(\"'\", \"''\")\n",
    "                row_values.append(f\"'{escaped_value}'\")\n",
    "            else:  # Handle non-string values (int, float, etc.)\n",
    "                row_values.append(str(value))\n",
    "        # Join row values with commas and wrap in parentheses\n",
    "        values.append(f\"({', '.join(row_values)})\")\n",
    "    # Join all rows with commas and end with a semicolon\n",
    "    insert_into_sql += \",\\n\".join(values) + \";\\n\"\n",
    "    \n",
    "    # Combine CREATE TABLE and INSERT INTO statements\n",
    "    return create_table_sql + insert_into_sql\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "def process_multiple_csv_files(input_directory, output_directory):\n",
    "    \"\"\"Process all CSV files in a directory and generate SQL scripts.\"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            csv_file = os.path.join(input_directory, file_name)\n",
    "            table_name = os.path.splitext(file_name)[0]  # Use file name (without extension) as table name\n",
    "            sql_script = generate_sql_from_csv(csv_file, table_name)\n",
    "            \n",
    "            # Save the SQL script to a corresponding .sql file\n",
    "            output_file = os.path.join(output_directory, f\"{table_name}.sql\")\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(sql_script)\n",
    "            \n",
    "            print(f\"SQL script for '{table_name}' generated and saved to '{output_file}'.\")\n",
    "\n",
    "# Example Usage\n",
    "input_directory = \"/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data\"  # Replace 'data' with the actual folder containing CSV files\n",
    "output_directory = \"/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data_sql\"  # Directory to save SQL files\n",
    "process_multiple_csv_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL script for 'part_categories' generated.\n",
      "SQL script for 'inventories' generated.\n",
      "SQL script for 'parts' generated.\n",
      "SQL script for 'inventory_sets' generated.\n",
      "SQL script for 'inventory_parts' generated.\n",
      "SQL script for 'colors' generated.\n",
      "SQL script for 'sets' generated.\n",
      "SQL script for 'themes' generated.\n",
      "All SQL scripts combined and saved to '/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/all_tables.sql'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_multiple_csv_files_into_one(input_directory, output_file):\n",
    "    \"\"\"Process all CSV files in a directory and generate a single SQL script.\"\"\"\n",
    "    all_sql_scripts = []\n",
    "\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            csv_file = os.path.join(input_directory, file_name)\n",
    "            table_name = os.path.splitext(file_name)[0]  # Use file name (without extension) as table name\n",
    "            sql_script = generate_sql_from_csv(csv_file, table_name)\n",
    "            \n",
    "            # Add this script to the list\n",
    "            all_sql_scripts.append(sql_script)\n",
    "            print(f\"SQL script for '{table_name}' generated.\")\n",
    "\n",
    "    # Combine all SQL scripts into one\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\\n\".join(all_sql_scripts))  # Add newlines between scripts\n",
    "    \n",
    "    print(f\"All SQL scripts combined and saved to '{output_file}'.\")\n",
    "\n",
    "# Example Usage\n",
    "input_directory = \"/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/data\"\n",
    "output_file = \"/Users/kevintr/Documents/ds_portfolio/ds_portfolio_github-repo/lego_analysis_challenge-main/all_tables.sql\"\n",
    "\n",
    "process_multiple_csv_files_into_one(input_directory, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
