---
title: "autism_children_ffnn"
author: "Kevin. T"
date: "2024-02-09"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(keras)
```


```{r}
library(tensorflow)
```


```{r}
devtools::install_github("rstudio/keras", dependencies = TRUE)
devtools::install_github("rstudio/tensorflow", dependencies = TRUE)

install_keras()
install_tensorflow()
```



## R Markdown

```{r}
library(tensorflow)
install_tensorflow(envname = "r-tensorflow")
```


This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
autism_df <- read_csv("autism_child_data.csv")
head(autism_df)
```

```{r}
summary(autism_df)
```
- age is in chr
- missing values
- encode catagorical variables to dummy for NN/ 

## Including Plots

You can also embed plots, for example:

```{r}
autism_df <- autism_df %>%
  mutate(
    relation = na_if(relation, "?"),
    ethnicity = na_if(ethnicity, "?")
  )

autism_df <- autism_df %>%
  mutate(age = as.integer(age),
         gender = as.factor((gender))) %>%
  na.omit()
# Assuming your dataframe is named autism_df
autism_df$gender <- ifelse(autism_df$gender == "m", 1, 0)  
autism_df$jundice <- ifelse(autism_df$jundice == "yes", 1, 0)
autism_df$austim <- ifelse(autism_df$austim == "yes", 1, 0)
autism_df$used_app_before <- ifelse(autism_df$used_app_before == "yes", 1, 0)
autism_df$`Class/ASD` <- ifelse(autism_df$`Class/ASD` == "YES", 1, 0)  # Pay attention to case sensitivity

str(autism_df)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
# Using the fastDummies package
library(fastDummies)

autism_dummy <- dummy_cols(autism_df, select_columns = c("ethnicity", "relation"), remove_first_dummy = TRUE)

write_csv(autism_df, "autism_new.csv")
write_csv(autism_dummy, "autism_encoded.csv")


```


ML Project
```{r}
set.seed(123)
autism_df <- as_tibble(read_csv("autism_encoded.csv"))
head(autism_df)
```


```{r}

# Assuming 'data' is your dataframe
autism_new <-autism_df %>%
  select(-id, -contry_of_res, -age_desc, -ethnicity, -relation) %>%
  mutate(across(where(is.numeric), scale))

# Note: Make sure 'Class_ASD' or your target variable is not scaled if included accidentally
```


```{r}
# split dataset
n = nrow(autism_new)
index <- sample(1:n,n*0.6)
autism_new%>%
  slice(index) -> autism_train
autism_new%>%
slice(-index) -> autism_valid
```





```{r}
p <- 29
K2 = 5 # Number of neurons in hidden laye
m = 2

# Adjusting the model for binary classification
NN1 <- keras_model_sequential() %>%
  layer_dense(units = K2, activation = "relu", input_shape = c(p)) %>%
  layer_dense(units = 1, activation = 'sigmoid')  # Adjusted for binary classification

NN1 %>% compile(
  loss = 'binary_crossentropy',  # Adjusted for binary classification
  optimizer = 'adam',
  metrics = c('accuracy')
)


```


```{r}
autism_df2 <- as_tibble(read_csv("autism_new.csv"))

x_scaled <- autism_df2 %>% 
  select(A1_Score:A10_Score) %>%
  scale() 

y_data <- autism_df2 %>%
  select(`Class/ASD`) 
```


```{r}
# Check dimensions
dim(x_scaled_train)
dim(y_data_train)
dim(x_scaled_valid)
dim(y_data_valid)

```


```{r}
set.seed(123)
n = nrow(autism_df2)
index <- 1:190
indexval = setdiff(1:n,index)
#Training sample
y_data_train = y_data[index,]
x_scaled_train = x_scaled[index,]
#Validation sample
y_data_valid = y_data[indexval,]
x_scaled_valid = x_scaled[indexval,]

```


```{r}


```




```{r}
p = 10  # Number of predictors
K2 = 5 # Number of neurons in hidden layer
m = 2  # Number of classes
NN1 <- keras_model_sequential () %>%
  layer_dense(units = K2, activation = "relu",input_shape = p) %>%
  layer_dense(units = m,activation = 'sigmoid') #Output layer
```


```{r}
NN1%>%  
    compile(
    loss = "binary_crossentropy")->NN1
```


```{r}
set.seed(123)
n = nrow(autism_df2)
index <- sample(1:n, n * 0.6)

# Prepare the training and validation data
x_scaled_train = x_scaled[index, ]
x_scaled_valid = x_scaled[-index, ]

y_data_train = y_data[index, ]
y_data_valid = y_data[-index, ]

p = 10  # Number of predictors
K2 = 5   # Number of neurons in the hidden layer
m = 1    # One output unit for binary classification

# Define the model
NN1 <- keras_model_sequential() %>%
  layer_dense(units = K2, activation = "relu", input_shape = c(p)) %>%
  layer_dense(units = m, activation = 'sigmoid')  # Output layer for binary classification

# Compile the model
NN1 %>% compile(
  loss = "binary_crossentropy",
  optimizer = 'adam',
  metrics = c('accuracy')
)

# Convert x_scaled_train and x_scaled_valid to matrices
x_scaled_train_matrix <- as.matrix(x_scaled_train)
x_scaled_valid_matrix <- as.matrix(x_scaled_valid)

# Ensure y_data_train and y_data_valid are numeric vectors
y_data_train_vector <- as.numeric(y_data_train$`Class/ASD`) - 1
y_data_valid_vector <- as.numeric(y_data_valid$`Class/ASD`) - 1

# Train the model using matrices
learnNN1 <- NN1 %>% fit(
  x = x_scaled_train_matrix,
  y = y_data_train_vector,
  epochs = 200,
  batch_size = 10,
  validation_data = list(x_scaled_valid_matrix, y_data_valid_vector),
  verbose = TRUE
)


```


```{r}
plot(learnNN1)
```

```{r}
x_scaled_matrix <- as.matrix(x_scaled)
Prob = predict(NN1 ,x_scaled_matrix)[,1]
```

```{r}
Prob_valid = predict(NN1 ,x_scaled_valid_matrix)
# This gives us the probability of each class. The predicted class is the one that
# is assigned the most probability. We can compute it as
Predicted_class = apply(Prob_valid,1,which.max)
```

```{r}
library(caret)
CM1 = confusionMatrix(as.factor(Predicted_class),
                      as.factor(autism_df2$`Class/ASD`[100:240]))
# This neural network has an accuracy of
CM1$overall[1]
```


```{r}
X_train <- as.matrix(autism_train[, -ncol(autism_train)])  # Exclude the last column (target)
X_valid <- as.matrix(autism_valid[, -ncol(autism_valid)]) 

# Prepare the target (y) - Assuming the last column is the binary target
# Convert to factor to ensure correct one-hot encoding
# Assuming the target variable is the last column and binary
y_train <- as.numeric(autism_train[[ncol(autism_train)]])  # Convert to numeric vector
y_valid <- as.numeric(autism_valid[[ncol(autism_valid)]])



```


```{r}
# Assuming NN1 is your compiled model
history <- NN1 %>% fit(
  x = X_train,
  y = y_train,
  epochs = 200,
  batch_size = 10,
  validation_data = list(X_valid, y_valid),
  verbose = TRUE
)


```



```{r}
# Assuming NN1 is your compiled model
learnNN1 <- NN1 %>% fit(
  x = Xscaled_train,               # Scaled features for training
  y = y_data_oneh_train,           # Target variable for training, one-hot encoded if necessary
  epochs = 200,                    # Number of epochs to train
  batch_size = 10,                 # Batch size for training
  validation_data = list(Xscaled_valid, y_data_oneh_valid), # Validation data
  verbose = TRUE                   # Set verbose to TRUE to see progress, or FALSE for no output
)

# Save the trained model to an HDF5 file
save_model_hdf5(NN1, "NN1.h5")

```

```{r}
get_weights(NN1)
```


```{r}
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

x_train <- as.matrix(autism_train[, -ncol(autism_train)])  # Exclude the target variable
y_train <- as.matrix(autism_train[, ncol(autism_train)])    # Only the target variable

x_valid <- as.matrix(autism_valid[, -ncol(autism_valid)])  # Exclude the target variable
y_valid <- as.matrix(autism_valid[, ncol(autism_valid)])    # Only the target variable

history <- model %>% fit(
  x_train, y_train,
  epochs = 30,
  batch_size = 10,
  validation_data = list(x_valid, y_valid)
)

```


```{r}
set.seed(123)
# Let us load the data set:
Tshirt = as_tibble(read.csv("Tshirt.csv"))
# Remember from the lecture that we must prepare the data when calibrating neural networks.
# First, we must make the response variable into one-hot format.
library(keras)
# In the function `to_categorical`, the indexes in the categorical variable must go
# from zero to m-1
y_data_oneh=to_categorical(Tshirt$Purchased_product-1, num_classes = 3)
# Second, we must scale the predictors:
Xscaled = scale(
Tshirt%>%
select(Price_cheap,Price_middle,Price_expensive,Age))
# Finally, let us split the sample into training and validation sample.
n = nrow(Tshirt)
index <- 1:300
indexval = setdiff(1:n,index)
#Training sample
y_data_oneh_train = y_data_oneh[index,]
Xscaled_train = Xscaled[index,]
#Validation sample
y_data_oneh_valid = y_data_oneh[indexval,]
Xscaled_valid = Xscaled[indexval,]
# Define the architecture of the neural network:
p = 4 # Number of predictors
m = 3 # Number of classes
K2 = 5 # Number of neuron in second layer
NN1 <- keras_model_sequential () %>%
layer_dense(units = K2, activation = "relu",input_shape = p) %>%
layer_dense(units = m,activation = 'softmax') #Output layer
# Define the loss function
NN1%>%
compile(
loss = "categorical_crossentropy")->NN1
```

```{r}
# Fit the model to the training data
history <- NN1 %>% fit(
  x = Xscaled_train,
  y = y_data_oneh_train,
  epochs = 100,
  batch_size = 32,
  validation_data = list(Xscaled_valid, y_data_oneh_valid)
)


```

```{r}

```

