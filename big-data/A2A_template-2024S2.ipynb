{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Assignment 2A : Building Models for eCommerce Fraud Detection\n",
    "\n",
    "## Table of Contents\n",
    "*  \n",
    "    * [Part 1 : Data Loading, Transformation and Exploration](#part-1)\n",
    "    * [Part 2 : Feature extraction and ML training](#part-2)\n",
    "    * [Part 3 : Customer Segmentation and Knowledge sharing with K-Mean](#part-3)\n",
    "    * [Part 4 : Data Ethics, Privacy, and Security](#part-4)\n",
    " \n",
    "Please add code/markdown cells if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Loading, Transformation and Exploration <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Data Loading\n",
    "In this section, you must load the given datasets into PySpark DataFrames and use DataFrame functions to process the data. Spark SQL usage is discouraged, and you can only use pandas to format results. For plotting, various visualisation packages can be used, but please ensure that you have included instructions to install the additional packages and that the installation will be successful in the provided docker container (in case your marker needs to clear the notebook and rerun it).\n",
    "\n",
    "### 1.1.1 Data Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.1.1 Write the code to create a SparkSession. For creating the SparkSession, you need to use a SparkConf object to configure the Spark app with a proper application name, to ensure the maximum partition size does not exceed 16MB, and to run locally with all CPU cores on your machine (note: if you have insufficient RAM, reducing the number of cores is acceptable.)  (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "master = \"local[*]\"\n",
    "app_name = \"FIT5202 A2A\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Write code to define the schemas for the category, customer, product, browsing behaviour and transaction datasets, following the data types suggested in the metadata file. (3%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "category_schema = StructType([ \n",
    "    StructField(\"category_id\", IntegerType(), True), \n",
    "    StructField(\"cat_level1\", StringType(), True), \n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True)\n",
    "])\n",
    "\n",
    "customer_schema = StructType([ \n",
    "    StructField(\"customer_id\", IntegerType(), True), \n",
    "    StructField(\"first_name\", StringType(), True), \n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthdate\", DateType(), True),\n",
    "    StructField(\"first_join_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "product_schema = StructType([ \n",
    "    StructField(\"id\", IntegerType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "browsing_behaviour_schema = StructType([ \n",
    "    StructField(\"session_id\", StringType(), True), \n",
    "    StructField(\"event_type\", StringType(), True), \n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),  \n",
    "    StructField(\"transaction_id\", StringType(), True),  \n",
    "    StructField(\"session_id\", StringType(), True),  \n",
    "    StructField(\"product_metadata\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"promo_amount\", DoubleType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"shipment_fee\", DoubleType(), True),\n",
    "    StructField(\"shipment_location_lat\", DoubleType(), True),\n",
    "    StructField(\"shipment_location_long\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"clear_payment\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 Using predefined schemas, write code to load the CSV files into separate data frames. Print the schemas of all data frames. (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(category_schema)\\\n",
    "    .load('category.csv')\n",
    "\n",
    "df_customers = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(customer_schema)\\\n",
    "    .load('customer.csv')\n",
    "\n",
    "df_transactions = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(transaction_schema)\\\n",
    "    .load('transactions.csv')\n",
    "\n",
    "df_product = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(product_schema)\\\n",
    "    .load('product.csv')\n",
    "\n",
    "df_browsing_behaviour = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(browsing_behaviour_schema)\\\n",
    "    .load('browsing_behaviour.csv')\n",
    "\n",
    "df_customer_session = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .load('customer_session.csv')\n",
    "\n",
    "df_fraud_transactions = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .load('fraud_transaction.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: double (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: double (nullable = true)\n",
      " |-- shipment_location_lat: double (nullable = true)\n",
      " |-- shipment_location_long: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- clear_payment: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- is_fraud: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_category.printSchema()\n",
    "df_customers.printSchema()\n",
    "df_transactions.printSchema()\n",
    "df_product.printSchema()\n",
    "df_browsing_behaviour.printSchema()\n",
    "df_customer_session.printSchema()\n",
    "df_fraud_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Transformation to Create Features <a class=\"anchor\" name=\"1.2\"></a>\n",
    "In the browsing behaviour dataset, there are 10 types of events:  \n",
    "VC(Viewing Category), VI(Viewing Item), VP(Viewing Promotion), AP(Add Promotion), CL(Click on a product/category) , ATC(Add a product to Shopping Cart), CO(CheckOut), HP(View HomePage), SCR(Mouse Scrolling), SER(Search for a product/category)  \n",
    "We categorise them into three different levels:  \n",
    "L1(actions that are highly likely lead to a purchase): AP, ATC, CO  \n",
    "L2(actions may lead to purchase): VC, VP, VI, SER  \n",
    "L3(not very important - just browsing):  SCR, HP, CL  \n",
    "Perform the following tasks based on the loaded data frames and create a new data frame.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1 For each transaction (linked to a browsing session), count the number of actions in each level and create 3 columns(L1_count, L2_count, L3_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------+--------+--------+\n",
      "|transaction_id                      |L1_count|L2_count|L3_count|\n",
      "+------------------------------------+--------+--------+--------+\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|7       |4       |9       |\n",
      "|37280714-c8e9-45d0-b089-b161435d27df|3       |11      |19      |\n",
      "|763e6b0a-faa6-4814-9ec6-a2cdfdd2ddc1|2       |3       |4       |\n",
      "|1b412535-32c7-4db4-97b2-67a6f5f56b6e|9       |8       |12      |\n",
      "|9144d238-525a-4833-87af-7e92a6ce8eaf|4       |8       |16      |\n",
      "|303c188e-3cfc-4990-bcf6-9c28ec92834f|5       |0       |2       |\n",
      "|74dce22c-d1ef-4f18-a4bb-fc4e658ef178|4       |1       |2       |\n",
      "|57de19b5-ee84-4726-88a8-07bca4d0b8b3|3       |1       |4       |\n",
      "|2755eefe-f1fb-4f23-a6aa-a7e53f573311|7       |6       |5       |\n",
      "|f3c01706-2016-4614-9164-3675f35ad75c|3       |0       |2       |\n",
      "|c5dc7f18-0276-41f9-aa02-50d1503bdf51|3       |2       |5       |\n",
      "|3a282187-656e-4ed2-9922-a4410159df08|2       |45      |44      |\n",
      "|498446e5-8576-494b-8b2f-ac11b3c67e53|3       |1       |1       |\n",
      "|1784b8e1-2198-42c2-8c16-0b47f2808292|4       |1       |1       |\n",
      "|4ba84a84-b8c3-4808-88e4-7a2260ee9a8f|2       |3       |9       |\n",
      "|7e77079e-8c0b-4f08-b366-7d5af13249af|4       |1       |4       |\n",
      "|2c29716f-2a50-4cf5-9b55-8be603bbc291|2       |3       |1       |\n",
      "|379f4287-fb17-4cef-ae0f-ff708ac16268|3       |0       |9       |\n",
      "|44ebd183-5c17-4b1f-acb9-3f5c41bce7e3|4       |2       |4       |\n",
      "|53ccbc90-8886-4e57-b1bc-9abe88cb63bb|3       |4       |3       |\n",
      "+------------------------------------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# first join transaction and browsing session with 'session_id'. \n",
    "df_transbrows_session = df_transactions.join(df_browsing_behaviour, on=\"session_id\", how=\"left\")\n",
    "# df_transbrows_session.select(\"transaction_id\", \"session_id\", \"event_type\").show()\n",
    "\n",
    "L1_events = ['AP', 'ATC', 'CO']\n",
    "L2_events = ['VC', 'VP', 'VI', 'SER']\n",
    "L3_events = ['SCR', 'HP', 'CL']\n",
    "\n",
    "df_transbrows_session = df_transbrows_session.withColumn(\n",
    "    \"event_level\", \n",
    "    F.when(F.col(\"event_type\").isin(L1_events), \"L1\")\n",
    "     .when(F.col(\"event_type\").isin(L2_events), \"L2\")\n",
    "     .when(F.col(\"event_type\").isin(L3_events), \"L3\")\n",
    "     .otherwise(\"Unknown\")\n",
    ")\n",
    "df_transbrows_session_agg = df_transbrows_session.groupBy(\"transaction_id\").agg(\n",
    "    F.sum(F.when(F.col(\"event_level\") == \"L1\", 1).otherwise(0)).alias(\"L1_count\"),\n",
    "    F.sum(F.when(F.col(\"event_level\") == \"L2\", 1).otherwise(0)).alias(\"L2_count\"),\n",
    "    F.sum(F.when(F.col(\"event_level\") == \"L3\", 1).otherwise(0)).alias(\"L3_count\")\n",
    ")\n",
    "df_transbrows_session_agg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+-----------+\n",
      "|transaction_id                      |event_type|event_level|\n",
      "+------------------------------------+----------+-----------+\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|HP        |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|ATC       |L1         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|HP        |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|CL        |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|CL        |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|ATC       |L1         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|ATC       |L1         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|ATC       |L1         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|SCR       |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|HP        |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|ATC       |L1         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|HP        |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|ATC       |L1         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|CL        |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|SCR       |L3         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|VI        |L2         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|VI        |L2         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|VI        |L2         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|VP        |L2         |\n",
      "|6211f29a-8435-4e22-a56c-0d8e5a114e48|CO        |L1         |\n",
      "+------------------------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking if aggregation done right by using sample transaction id\n",
    "df_test = df_transbrows_session.filter(F.col(\"transaction_id\") == \"6211f29a-8435-4e22-a56c-0d8e5a114e48\")\n",
    "df_test.select(\"transaction_id\", \"event_type\", \"event_level\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 Create two columns with a percentage ratio of L1 and L2 actions. (i.e. L1 ratio = L1/(L1+L2+L3) * 100%)\n",
    "\n",
    "L1 ratio = L1/(L1+L2+L3) * 100% \\\n",
    "L2 ratio = L2/(L1+L2+L3) * 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+--------+------------------+------------------+\n",
      "|      transaction_id|L1_count|L2_count|L3_count|          L1_ratio|          L2_ratio|\n",
      "+--------------------+--------+--------+--------+------------------+------------------+\n",
      "|6211f29a-8435-4e2...|       7|       4|       9|              35.0|              20.0|\n",
      "|37280714-c8e9-45d...|       3|      11|      19| 9.090909090909092| 33.33333333333333|\n",
      "|763e6b0a-faa6-481...|       2|       3|       4| 22.22222222222222| 33.33333333333333|\n",
      "|1b412535-32c7-4db...|       9|       8|      12| 31.03448275862069|27.586206896551722|\n",
      "|9144d238-525a-483...|       4|       8|      16|14.285714285714285| 28.57142857142857|\n",
      "|303c188e-3cfc-499...|       5|       0|       2| 71.42857142857143|               0.0|\n",
      "|74dce22c-d1ef-4f1...|       4|       1|       2| 57.14285714285714|14.285714285714285|\n",
      "|57de19b5-ee84-472...|       3|       1|       4|              37.5|              12.5|\n",
      "|2755eefe-f1fb-4f2...|       7|       6|       5| 38.88888888888889| 33.33333333333333|\n",
      "|f3c01706-2016-461...|       3|       0|       2|              60.0|               0.0|\n",
      "|c5dc7f18-0276-41f...|       3|       2|       5|              30.0|              20.0|\n",
      "|3a282187-656e-4ed...|       2|      45|      44| 2.197802197802198| 49.45054945054945|\n",
      "|498446e5-8576-494...|       3|       1|       1|              60.0|              20.0|\n",
      "|1784b8e1-2198-42c...|       4|       1|       1| 66.66666666666666|16.666666666666664|\n",
      "|4ba84a84-b8c3-480...|       2|       3|       9|14.285714285714285|21.428571428571427|\n",
      "|7e77079e-8c0b-4f0...|       4|       1|       4| 44.44444444444444| 11.11111111111111|\n",
      "|2c29716f-2a50-4cf...|       2|       3|       1| 33.33333333333333|              50.0|\n",
      "|379f4287-fb17-4ce...|       3|       0|       9|              25.0|               0.0|\n",
      "|44ebd183-5c17-4b1...|       4|       2|       4|              40.0|              20.0|\n",
      "|53ccbc90-8886-4e5...|       3|       4|       3|              30.0|              40.0|\n",
      "+--------------------+--------+--------+--------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_event_ratio = df_transbrows_session_agg.withColumn(\"L1_ratio\", (F.col(\"L1_count\") / (F.col(\"L1_count\") + F.col(\"L2_count\") + F.col(\"L3_count\"))) * 100\n",
    ").withColumn( \"L2_ratio\", (F.col(\"L2_count\") / (F.col(\"L1_count\") + F.col(\"L2_count\") + F.col(\"L3_count\"))) * 100\n",
    ")\n",
    "\n",
    "df_with_ratios.select(\"transaction_id\", \"L1_count\", \"L2_count\", \"L3_count\", \"L1_ratio\", \"L2_ratio\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3 For each unique browsing session, based on event_time, extract the time of day as 4 groups: morning(6am-11:59am), afternoon(12pm-5:59pm), evening(6pm-11:59pm), night(12am-5:59am), add a column. (note: use medium time if a browsing session spans across different groups. For example, if a session starts at 10 am and ends at 1 pm, use 11:30 => (10+13)/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browsing_session\n",
    "\n",
    "def extract_time(event_time):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------------+\n",
      "|session_id                          |event_time             |\n",
      "+------------------------------------+-----------------------+\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-11-14 13:02:39.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-11-25 11:01:01.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-11-25 11:17:01.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-12-06 09:07:47.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-12-06 09:12:13.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-12-17 07:04:52.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-12-17 07:12:08.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2020-12-28 04:49:04.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2021-01-08 02:28:32.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2021-01-19 00:17:38.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2021-01-29 22:34:04.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2021-02-09 20:00:24.331|\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|2021-03-25 11:43:01.331|\n",
      "+------------------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = df_browsing_behaviour.filter(F.col(\"session_id\") == \"c9718135-8134-42b2-8e1e-2737fd6b49b1\")\n",
    "df_test.select(\"session_id\", \"event_time\").orderBy(\"event_time\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+-----------------------+-----------------------+\n",
      "|session_id                          |event_date|min_time               |max_time               |\n",
      "+------------------------------------+----------+-----------------------+-----------------------+\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-04-25|2020-04-25 04:32:54.612|2020-04-25 04:36:02.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-05-10|2020-05-10 07:53:43.612|2020-05-10 07:53:43.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-05-06|2020-05-06 00:01:57.612|2020-05-06 00:01:57.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-05-07|2020-05-07 01:52:38.612|2020-05-07 01:52:38.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-05-04|2020-05-04 22:04:00.612|2020-05-04 22:04:00.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-05-03|2020-05-03 19:56:51.612|2020-05-03 19:56:51.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-05-02|2020-05-02 17:53:16.612|2020-05-02 17:53:16.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-05-01|2020-05-01 15:45:42.612|2020-05-01 15:45:42.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-04-30|2020-04-30 13:59:45.612|2020-04-30 13:59:45.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-04-29|2020-04-29 12:00:25.612|2020-04-29 12:00:25.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-04-28|2020-04-28 10:06:14.612|2020-04-28 10:06:14.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-04-27|2020-04-27 08:16:02.612|2020-04-27 08:16:02.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-04-26|2020-04-26 06:19:34.612|2020-04-26 06:27:30.612|\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|2020-04-24|2020-04-24 02:35:21.612|2020-04-24 02:35:21.612|\n",
      "+------------------------------------+----------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Extract the date from the event_time\n",
    "df_transbrows_session = df_transbrows_session.withColumn(\"event_date\", F.to_date(F.col(\"event_time\")))\n",
    "\n",
    "# Step 2: Group by session_id and event_date, and calculate min and max event_time for each date\n",
    "df_session_time = df_transbrows_session.groupBy(\"session_id\", \"event_date\").agg(\n",
    "    F.min(\"event_time\").alias(\"min_time\"),\n",
    "    F.max(\"event_time\").alias(\"max_time\")\n",
    ")\n",
    "\n",
    "# Step 3: Filter for a specific session to check results\n",
    "df_test = df_session_time.filter(F.col(\"session_id\") == \"d31b9d4b-126a-49c4-be4b-8e5d7f70804d\")\n",
    "\n",
    "# Show the result\n",
    "df_test.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+-----------------------+--------------+-----------+\n",
      "|session_id                          |event_type|event_time             |traffic_source|device_type|\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|AP        |2020-12-17 07:04:52.331|MOBILE        |Android    |\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|CL        |2020-12-06 09:07:47.331|MOBILE        |Android    |\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|SER       |2020-12-17 07:12:08.331|MOBILE        |Android    |\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|CL        |2021-01-08 02:28:32.331|MOBILE        |Android    |\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|ATC       |2021-01-19 00:17:38.331|MOBILE        |Android    |\n",
      "|c9718135-8134-42b2-8e1e-2737fd6b49b1|VI        |2020-12-28 04:49:04.331|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|HP        |2020-02-23 23:03:14.612|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|ATC       |2020-02-24 08:56:58.612|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|ATC       |2020-02-24 18:52:26.612|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|ATC       |2020-02-25 04:46:38.612|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|ATC       |2020-02-25 14:43:26.612|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|ATC       |2020-02-26 00:35:01.612|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|ATC       |2020-02-26 10:27:38.612|MOBILE        |Android    |\n",
      "|d2077615-9026-4c5c-b088-a35519b64b05|ATC       |2020-02-24 08:56:37.612|MOBILE        |Android    |\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|HP        |2020-04-24 02:35:21.612|MOBILE        |Android    |\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|ATC       |2020-04-25 04:36:02.612|MOBILE        |Android    |\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|AP        |2020-04-26 06:27:30.612|MOBILE        |Android    |\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|SCR       |2020-04-26 06:19:34.612|MOBILE        |Android    |\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|CL        |2020-04-27 08:16:02.612|MOBILE        |Android    |\n",
      "|d31b9d4b-126a-49c4-be4b-8e5d7f70804d|CL        |2020-04-28 10:06:14.612|MOBILE        |Android    |\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_browsing_behaviour.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.4 Join data frames to find customer information and add columns to feature_df: gender, age, geolocation, first join year. (note: For some columns, you need to perform transformations. For age, keep the integer only by rounding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+----------------------+\n",
      "|customer_id|shipment_location_lat|shipment_location_long|\n",
      "+-----------+---------------------+----------------------+\n",
      "|14159      |-4.26351275671241    |105.489401701251      |\n",
      "|22576      |-7.91707661186231    |110.13187555325       |\n",
      "|18696      |-7.39661418330981    |109.511262594032      |\n",
      "|90136      |-0.637290541399757   |109.492521253314      |\n",
      "|18960      |-7.32004136393024    |111.225797135699      |\n",
      "|60646      |-4.52328589944563    |105.385799510518      |\n",
      "|5901       |-7.43210236666926    |111.096960686913      |\n",
      "|69072      |-6.26355191799179    |106.859716713089      |\n",
      "|92076      |-0.420556955342107   |113.93175424743       |\n",
      "|51799      |-2.98365414329573    |101.93104883237       |\n",
      "|32840      |-6.29321554243872    |106.891616710191      |\n",
      "|93365      |3.11080990817827     |99.5849034250288      |\n",
      "|6664       |-5.32866351581116    |105.431488069038      |\n",
      "|4343       |-6.35528560922436    |106.909932739596      |\n",
      "|17336      |-6.94019786467257    |107.362905837214      |\n",
      "|78585      |-7.91844917583557    |110.25810209077       |\n",
      "|60389      |-1.1739386014771     |101.860764201145      |\n",
      "|14664      |-4.4149464340499     |120.318300179706      |\n",
      "|12616      |-6.276181252986      |106.877757231913      |\n",
      "|79403      |-5.34533610036892    |105.645528090472      |\n",
      "+-----------+---------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine geolocation using shipment long lat\n",
    "df_geolocation = df_transactions.select(\"customer_id\", \"shipment_location_lat\", \"shipment_location_long\")\n",
    "df_geolocation.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garongan, Panjatan, Kulon Progo, Daerah Istimewa Yogyakarta, Jawa, Indonesia\n"
     ]
    }
   ],
   "source": [
    "location = geolocator.reverse(\"-7.91707661186231, 110.13187555325 \")\n",
    "print(location.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy - find customer address base don their shipment latlong\n",
    "from geopy.geocoders import Nominatim\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def find_address(shipment_location_lat, shipment_location_long):\n",
    "    geolocator = Nominatim(user_agent=\"findAddress\")\n",
    "    location = geolocator.reverse((shipment_location_lat, shipment_location_long))\n",
    "    return location.address if location else None  \n",
    "\n",
    "address_udf = udf(find_address,StringType())\n",
    "df_geolocation2 = df_geolocation.withColumn('address', address_udf('shipment_location_lat', 'shipment_location_long'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|customer_id|shipment_location_lat|shipment_location_long|address                                                                                                                                                   |\n",
      "+-----------+---------------------+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|14159      |-4.26351275671241    |105.489401701251      |Marga Jaya, Tulang Bawang, Lampung, Sumatera, Indonesia                                                                                                   |\n",
      "|22576      |-7.91707661186231    |110.13187555325       |Garongan, Panjatan, Kulon Progo, Daerah Istimewa Yogyakarta, Jawa, Indonesia                                                                              |\n",
      "|18696      |-7.39661418330981    |109.511262594032      |Kejobong, Purbalingga, Jawa Tengah, Jawa, Indonesia                                                                                                       |\n",
      "|90136      |-0.637290541399757   |109.492521253314      |Nipah Panjang, Batu Ampar, Kubu Raya, Kalimantan Barat, Kalimantan, Indonesia                                                                             |\n",
      "|18960      |-7.32004136393024    |111.225797135699      |Mengger, Ngawi, Jawa Timur, Jawa, Indonesia                                                                                                               |\n",
      "|60646      |-4.52328589944563    |105.385799510518      |Lampung Tengah, Lampung, Sumatera, 34162, Indonesia                                                                                                       |\n",
      "|5901       |-7.43210236666926    |111.096960686913      |Kaliwedi, Sragen, Jawa Tengah, Jawa, 57254, Indonesia                                                                                                     |\n",
      "|69072      |-6.26355191799179    |106.859716713089      |Sekretariat RW 06 Kelurahan Cililitan, Jalan Ciliwung, RW 06, Cililitan, Kramat Jati, Jakarta Timur, Daerah Khusus ibukota Jakarta, Jawa, 13640, Indonesia|\n",
      "|92076      |-0.420556955342107   |113.93175424743       |Kapuas, Kalimantan Tengah, Kalimantan, Indonesia                                                                                                          |\n",
      "|51799      |-2.98365414329573    |101.93104883237       |Bengkulu Utara, Bengkulu, Sumatera, Indonesia                                                                                                             |\n",
      "|32840      |-6.29321554243872    |106.891616710191      |RW 04, Lubang Buaya, Cipayung, Jakarta Timur, Daerah Khusus ibukota Jakarta, Jawa, 13810, Indonesia                                                       |\n",
      "|93365      |3.11080990817827     |99.5849034250288      |Jalan Lintas Sumatera, Kisaran, Batu Bara, Sumatera Utara, Sumatera, 21211, Indonesia                                                                     |\n",
      "|6664       |-5.32866351581116    |105.431488069038      |Lampung Selatan, Lampung, Sumatera, Indonesia                                                                                                             |\n",
      "|4343       |-6.35528560922436    |106.909932739596      |Gang Amanah III, RW 02, Pondok Ranggon, Cipayung, Jakarta Timur, Daerah Khusus ibukota Jakarta, Jawa, 13850, Indonesia                                    |\n",
      "|17336      |-6.94019786467257    |107.362905837214      |Jalan Plta Sagulng, Sarinagen, Bandung Barat, Jawa Barat, Jawa, Indonesia                                                                                 |\n",
      "|78585      |-7.91844917583557    |110.25810209077       |Jalan Sentolo - Brosot, Pengkol, Gulurejo, Lendah, Kulon Progo, Daerah Istimewa Yogyakarta, Jawa, 55663, Indonesia                                        |\n",
      "|60389      |-1.1739386014771     |101.860764201145      |Bukit Sari, Bungo, Jambi, Sumatera, Indonesia                                                                                                             |\n",
      "|14664      |-4.4149464340499     |120.318300179706      |Bone, Mattoanging, Sulawesi Selatan, Sulawesi, Indonesia                                                                                                  |\n",
      "|12616      |-6.276181252986      |106.877757231913      |Jalan Haji Emuntipala I, RW 05, Makasar, Jakarta Timur, Daerah Khusus ibukota Jakarta, Jawa, 13650, Indonesia                                             |\n",
      "|79403      |-5.34533610036892    |105.645528090472      |Bukit Raya, Lampung Timur, Lampung, Sumatera, Indonesia                                                                                                   |\n",
      "+-----------+---------------------+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_geolocation2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.5 Join data frames to find out the number of purchases the customer has made, add a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.6 Attach the transaction labels for fraud/non-fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring the Data <a class=\"anchor\" name=\"1.3\"></a>\n",
    "**1.3.1 With the feature_df, write code to show the basic statistics: a) For each numeric column, show count, mean, stddev, min, max, 25 percentile, 50 percentile, 75 percentile; b) For each non-numeric column, display the top-5 values and the corresponding counts; c) For each boolean column, display the value and count. (3%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2 Explore the dataframe and write code to present two plots worthy of presentation to the company, describe your plots and discuss the findings from the plots. (8%)**\n",
    "One of the plots needs to be based on feature_df in regard to fraudulent behaviour; you’re free to choose the other one.  \n",
    "Hint 1: You can use basic plots (e.g., histograms, line charts, scatter plots) to show the relationship between a column and the label or more advanced plots like correlation plots.  \n",
    "Hint 2: If your data is too large for plotting, consider using sampling before plotting.  \n",
    "150 words max for each plot’s description and discussion  \n",
    "Feel free to use any plotting libraries: matplotlib, seabon, plotly, etc.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Feature extraction and ML training <a class=\"anchor\" name=\"part-2\"></a>\n",
    "In this section, you must use PySpark DataFrame functions and ML packages for data preparation, model building, and evaluation. Other ML packages, such as scikit-learn, would receive zero marks.\n",
    "### 2.1 Discuss the feature selection and prepare the feature columns\n",
    "\n",
    "2.1.1 Based on the data exploration from 1.2 and considering the use case, discuss the importance of those features (For example, which features may be useless and should be removed, which feature has a significant impact on the label column, which should be transformed), which features you are planning to use? Discuss the reasons for selecting them and how you create/transform them\n",
    "300 words max for the discussion\n",
    "Please only use the provided data for model building\n",
    "You can create/add additional features based on the dataset\n",
    "Hint - Use the insights from the data exploration/domain knowledge/statistical models to consider whether to create more feature columns, whether to remove some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Write code to create/transform the columns based on your discussion above\n",
    "Hint: You can use one data frame for both use cases (classification and k-mean later in part 3) since you can select your desired columns as the input and output for each use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing Spark ML Transformers/Estimators for features, labels, and models  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "\n",
    "**2.2.1 Write code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1 and create ML model Estimators for Random Forest (RF) and Gradient-boosted tree (GBT) model.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2. Write code to include the above Transformers/Estimators into two pipelines.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the training data and testing data  \n",
    "Write code to split the data for training and testing purposes.\n",
    "Note: Due to the large dataset size, you can use random sampling (say 20% of the dataset) and do a train/test split or use one year of data for training and another year for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and evaluating models  \n",
    "2.4.1 Write code to use the corresponding ML Pipelines to train the models on the training data from 2.3. And then use the trained models to predict the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 For both models (RF and GBT) and testing data, write code to display the count of TP/TN/FP/FN. Compute the AUC, accuracy, recall, and precision for the above-threshold/below-threshold label from each model testing result using PySpark MLlib/ML APIs.\n",
    "Draw a ROC plot.\n",
    "Discuss which one is the better model (no word limit; please keep it concise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 Save the better model (you need it for Part B of Assignment 2).\n",
    "(Note: You may need to go through a few training loops or use more data to create a better-performing model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Customer Clustering and Knowledge sharing with K-Mean <a class=\"anchor\" name=\"part-3\"></a>  \n",
    "Please see the specification for this task and add code/markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Data Ethics, Privacy, and Security <a class=\"anchor\" name=\"part-4\"></a>  \n",
    "Please see the specification for this task and add markdown cells(word limit: 500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "Please add your references below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
