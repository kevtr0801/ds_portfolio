{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Assignment 2A : Building Models for eCommerce Fraud Detection\n",
    "\n",
    "## Table of Contents\n",
    "*  \n",
    "    * [Part 1 : Data Loading, Transformation and Exploration](#part-1)\n",
    "    * [Part 2 : Feature extraction and ML training](#part-2)\n",
    "    * [Part 3 : Customer Segmentation and Knowledge sharing with K-Mean](#part-3)\n",
    "    * [Part 4 : Data Ethics, Privacy, and Security](#part-4)\n",
    " \n",
    "Please add code/markdown cells if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Loading, Transformation and Exploration <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Data Loading\n",
    "In this section, you must load the given datasets into PySpark DataFrames and use DataFrame functions to process the data. Spark SQL usage is discouraged, and you can only use pandas to format results. For plotting, various visualisation packages can be used, but please ensure that you have included instructions to install the additional packages and that the installation will be successful in the provided docker container (in case your marker needs to clear the notebook and rerun it).\n",
    "\n",
    "### 1.1.1 Data Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.1.1 Write the code to create a SparkSession. For creating the SparkSession, you need to use a SparkConf object to configure the Spark app with a proper application name, to ensure the maximum partition size does not exceed 16MB, and to run locally with all CPU cores on your machine (note: if you have insufficient RAM, reducing the number of cores is acceptable.)  (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "master = \"local[*]\"\n",
    "app_name = \"FIT5202 A2A\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Write code to define the schemas for the category, customer, product, browsing behaviour and transaction datasets, following the data types suggested in the metadata file. (3%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "category_schema = StructType([ \n",
    "    StructField(\"category_id\", IntegerType(), True), \n",
    "    StructField(\"cat_level1\", StringType(), True), \n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True)\n",
    "])\n",
    "\n",
    "customer_schema = StructType([ \n",
    "    StructField(\"customer_id\", IntegerType(), True), \n",
    "    StructField(\"first_name\", StringType(), True), \n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthdate\", DateType(), True),\n",
    "    StructField(\"first_join_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "product_schema = StructType([ \n",
    "    StructField(\"id\", IntegerType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "browsing_behaviour_schema = StructType([ \n",
    "    StructField(\"session_id\", StringType(), True), \n",
    "    StructField(\"event_type\", StringType(), True), \n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),  \n",
    "    StructField(\"transaction_id\", StringType(), True),  \n",
    "    StructField(\"session_id\", StringType(), True),  \n",
    "    StructField(\"product_metadata\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"promo_amount\", DoubleType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"shipment_fee\", DoubleType(), True),\n",
    "    StructField(\"shipment_location_lat\", DoubleType(), True),\n",
    "    StructField(\"shipment_location_long\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"clear_payment\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 Using predefined schemas, write code to load the CSV files into separate data frames. Print the schemas of all data frames. (2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(category_schema)\\\n",
    "    .load('category.csv')\n",
    "\n",
    "df_customers = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(customer_schema)\\\n",
    "    .load('customer.csv')\n",
    "\n",
    "df_transactions = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(transaction_schema)\\\n",
    "    .load('transactions.csv')\n",
    "\n",
    "df_product = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(product_schema)\\\n",
    "    .load('product.csv')\n",
    "\n",
    "df_browsing_behaviour = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .schema(browsing_behaviour_schema)\\\n",
    "    .load('browsing_behaviour.csv')\n",
    "\n",
    "df_customer_session = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .load('customer_session.csv')\n",
    "\n",
    "df_fraud_transactions = spark.read.format('csv')\\\n",
    "    .option('header', True).option('escape', '\"')\\\n",
    "    .load('fraud_transaction.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: double (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: double (nullable = true)\n",
      " |-- shipment_location_lat: double (nullable = true)\n",
      " |-- shipment_location_long: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- clear_payment: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- is_fraud: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_category.printSchema()\n",
    "df_customers.printSchema()\n",
    "df_transactions.printSchema()\n",
    "df_product.printSchema()\n",
    "df_browsing_behaviour.printSchema()\n",
    "df_customer_session.printSchema()\n",
    "df_fraud_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Transformation to Create Features <a class=\"anchor\" name=\"1.2\"></a>\n",
    "In the browsing behaviour dataset, there are 10 types of events:  \n",
    "VC(Viewing Category), VI(Viewing Item), VP(Viewing Promotion), AP(Add Promotion), CL(Click on a product/category) , ATC(Add a product to Shopping Cart), CO(CheckOut), HP(View HomePage), SCR(Mouse Scrolling), SER(Search for a product/category)  \n",
    "We categorise them into three different levels:  \n",
    "L1(actions that are highly likely lead to a purchase): AP, ATC, CO  \n",
    "L2(actions may lead to purchase): VC, VP, VI, SER  \n",
    "L3(not very important - just browsing):  SCR, HP, CL  \n",
    "Perform the following tasks based on the loaded data frames and create a new data frame.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1 For each transaction (linked to a browsing session), count the number of actions in each level and create 3 columns(L1_count, L2_count, L3_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# first join transaction and browsing session with 'session_id'. \n",
    "df_transbrows_session = df_transactions.join(df_browsing_behaviour, on=\"session_id\", how=\"left\")\n",
    "# df_transbrows_session.select(\"transaction_id\", \"session_id\", \"event_type\").show()\n",
    "\n",
    "L1_events = ['AP', 'ATC', 'CO']\n",
    "L2_events = ['VC', 'VP', 'VI', 'SER']\n",
    "L3_events = ['SCR', 'HP', 'CL']\n",
    "\n",
    "df_transbrows_session = df_transbrows_session.withColumn(\n",
    "    \"event_level\", \n",
    "    F.when(F.col(\"event_type\").isin(L1_events), \"L1\")\n",
    "     .when(F.col(\"event_type\").isin(L2_events), \"L2\")\n",
    "     .when(F.col(\"event_type\").isin(L3_events), \"L3\")\n",
    "     .otherwise(\"Unknown\")\n",
    ")\n",
    "df_transbrows_session_agg = df_transbrows_session.groupBy(\"transaction_id\").agg(\n",
    "    F.sum(F.when(F.col(\"event_level\") == \"L1\", 1).otherwise(0)).alias(\"L1_count\"),\n",
    "    F.sum(F.when(F.col(\"event_level\") == \"L2\", 1).otherwise(0)).alias(\"L2_count\"),\n",
    "    F.sum(F.when(F.col(\"event_level\") == \"L3\", 1).otherwise(0)).alias(\"L3_count\")\n",
    ")\n",
    "df_transbrows_session_agg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if aggregation done right by using sample transaction id\n",
    "df_test = df_transbrows_session.filter(F.col(\"transaction_id\") == \"6211f29a-8435-4e22-a56c-0d8e5a114e48\")\n",
    "df_test.select(\"transaction_id\", \"event_type\", \"event_level\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 Create two columns with a percentage ratio of L1 and L2 actions. (i.e. L1 ratio = L1/(L1+L2+L3) * 100%)\n",
    "\n",
    "L1 ratio = L1/(L1+L2+L3) * 100% \\\n",
    "L2 ratio = L2/(L1+L2+L3) * 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_ratio = df_transbrows_session_agg.withColumn(\"L1_ratio\", (F.col(\"L1_count\") / (F.col(\"L1_count\") + F.col(\"L2_count\") + F.col(\"L3_count\"))) * 100\n",
    ").withColumn( \"L2_ratio\", (F.col(\"L2_count\") / (F.col(\"L1_count\") + F.col(\"L2_count\") + F.col(\"L3_count\"))) * 100\n",
    ")\n",
    "\n",
    "df_event_ratio.select(\"transaction_id\", \"L1_count\", \"L2_count\", \"L3_count\", \"L1_ratio\", \"L2_ratio\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3 For each unique browsing session, based on event_time, extract the time of day as 4 groups: morning(6am-11:59am), afternoon(12pm-5:59pm), evening(6pm-11:59pm), night(12am-5:59am), add a column. (note: use medium time if a browsing session spans across different groups. For example, if a session starts at 10 am and ends at 1 pm, use 11:30 => (10+13)/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browsing_session\n",
    "\n",
    "def extract_time(event_time):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_browsing_behaviour.filter(F.col(\"session_id\") == \"c9718135-8134-42b2-8e1e-2737fd6b49b1\")\n",
    "df_test.select(\"session_id\", \"event_time\").orderBy(\"event_time\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Extract the date from the event_time\n",
    "df_transbrows_session = df_transbrows_session.withColumn(\"event_date\", F.to_date(F.col(\"event_time\")))\n",
    "\n",
    "# Step 2: Group by session_id and event_date, and calculate min and max event_time for each date\n",
    "df_session_time = df_transbrows_session.groupBy(\"session_id\", \"event_date\").agg(\n",
    "    F.min(\"event_time\").alias(\"min_time\"),\n",
    "    F.max(\"event_time\").alias(\"max_time\")\n",
    ")\n",
    "\n",
    "# Step 3: Filter for a specific session to check results\n",
    "df_test = df_session_time.filter(F.col(\"session_id\") == \"d31b9d4b-126a-49c4-be4b-8e5d7f70804d\")\n",
    "\n",
    "# Show the result\n",
    "df_test.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browsing_behaviour.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.4 Join data frames to find customer information and add columns to feature_df: gender, age, geolocation, first join year. (note: For some columns, you need to perform transformations. For age, keep the integer only by rounding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine geolocation using shipment long lat\n",
    "df_geolocation = df_transactions.select(\"customer_id\", \"shipment_location_lat\", \"shipment_location_long\")\n",
    "# df_geolocation.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy\n",
    "# pip install ratelimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy - find customer address base don their shipment latlong\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def find_address(shipment_location_lat, shipment_location_long):\n",
    "    geolocator = Nominatim(user_agent=\"findAddress\")\n",
    "    location = geolocator.reverse((shipment_location_lat, shipment_location_long),timeout=10)\n",
    "    return location.address if location else None  \n",
    "\n",
    "address_udf = udf(find_address,StringType())\n",
    "df_geolocation2 = df_geolocation.withColumn('geolocation', address_udf('shipment_location_lat', 'shipment_location_long'))\n",
    "#df_customers2 = df_customers.join(df_geolocation2, on=\"customer_id\", how=\"left\").select(\"customer_id\", \"birthdate\",\"first_join_date\",\"geolocation\",\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_geolocation2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_geolocation2.write.csv(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---+---------------+\n",
      "|customer_id|gender|age|first_join_year|\n",
      "+-----------+------+---+---------------+\n",
      "|       2870|     F| 28|           2019|\n",
      "|       8193|     F| 31|           2017|\n",
      "|       7279|     M| 35|           2020|\n",
      "|      88813|     M| 33|           2021|\n",
      "|      82542|     M| 24|           2021|\n",
      "|       5440|     F| 35|           2021|\n",
      "|      90319|     M| 34|           2019|\n",
      "|      96453|     F| 19|           2022|\n",
      "|       8031|     F| 28|           2019|\n",
      "|      61533|     M| 37|           2020|\n",
      "|      72203|     M| 42|           2017|\n",
      "|      74362|     F| 28|           2022|\n",
      "|       9152|     F| 28|           2019|\n",
      "|      22199|     M| 38|           2019|\n",
      "|      94370|     F| 26|           2018|\n",
      "|      73093|     F| 32|           2021|\n",
      "|      72106|     F| 23|           2021|\n",
      "|      97883|     F| 25|           2017|\n",
      "|       3434|     M| 26|           2017|\n",
      "|      31163|     F| 21|           2020|\n",
      "+-----------+------+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "df_customers2 = df_customers.withColumn('first_join_year', year(df_customers['first_join_date']))\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def age(birthdate):\n",
    "    birthdate = str(birthdate)\n",
    "    dob_date = datetime.strptime(birthdate, \"%Y-%m-%d\").date()\n",
    "    today = date.today()\n",
    "    return today.year - dob_date.year - ((today.month, today.day) < (dob_date.month, dob_date.day))\n",
    "age_udf = udf(age,IntegerType())\n",
    "df_customers_age = df_customers2.withColumn('age', age_udf('birthdate')).select('customer_id','gender','age', 'first_join_year')\n",
    "df_customers_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---+---------------+---------------------+----------------------+\n",
      "|customer_id|gender|age|first_join_year|shipment_location_lat|shipment_location_long|\n",
      "+-----------+------+---+---------------+---------------------+----------------------+\n",
      "|      14159|     F| 30|           2019|    -4.26351275671241|      105.489401701251|\n",
      "|      22576|     F| 32|           2020|    -7.91707661186231|       110.13187555325|\n",
      "|      18696|     F| 28|           2020|    -7.39661418330981|      109.511262594032|\n",
      "|      90136|     F| 24|           2017|   -0.637290541399757|      109.492521253314|\n",
      "|      18960|     F| 24|           2018|    -7.32004136393024|      111.225797135699|\n",
      "|      60646|     F| 25|           2018|    -4.52328589944563|      105.385799510518|\n",
      "|       5901|     F| 26|           2018|    -7.43210236666926|      111.096960686913|\n",
      "|      69072|     F| 22|           2017|    -6.26355191799179|      106.859716713089|\n",
      "|      92076|     F| 46|           2017|   -0.420556955342107|       113.93175424743|\n",
      "|      51799|     F| 34|           2018|    -2.98365414329573|       101.93104883237|\n",
      "|      32840|     F| 16|           2018|    -6.29321554243872|      106.891616710191|\n",
      "|      93365|     F| 35|           2020|     3.11080990817827|      99.5849034250288|\n",
      "|       6664|     F| 25|           2019|    -5.32866351581116|      105.431488069038|\n",
      "|       4343|     M| 20|           2021|    -6.35528560922436|      106.909932739596|\n",
      "|      17336|     F| 37|           2018|    -6.94019786467257|      107.362905837214|\n",
      "|      78585|     F| 20|           2019|    -7.91844917583557|       110.25810209077|\n",
      "|      60389|     F| 17|           2019|     -1.1739386014771|      101.860764201145|\n",
      "|      14664|     F| 28|           2016|     -4.4149464340499|      120.318300179706|\n",
      "|      12616|     F| 21|           2019|      -6.276181252986|      106.877757231913|\n",
      "|      79403|     F| 32|           2017|    -5.34533610036892|      105.645528090472|\n",
      "+-----------+------+---+---------------+---------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_geolocation = df_transactions.select(\"customer_id\", \"shipment_location_lat\", \"shipment_location_long\")\n",
    "df_custom_geo = df_customers_age.join(df_geolocation, on=\"customer_id\", how=\"inner\")\n",
    "df_custom_geo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy - find customer address base don their shipment latlong\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "#from geopy.extra.rate_limiter import RateLimiter\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def find_address(shipment_location_lat, shipment_location_long):\n",
    "    geolocator = Nominatim(user_agent=\"abcd\")\n",
    "    location = geolocator.reverse((shipment_location_lat, shipment_location_long),timeout=None)\n",
    "    return location.address if location else None  \n",
    "\n",
    "address_udf = udf(find_address,StringType())\n",
    "feature_df = df_custom_geo.withColumn('geolocation', address_udf('shipment_location_lat', 'shipment_location_long'))\n",
    "#feature_df = feature_df.select('gender', 'age', 'geolocation', 'first_join_year')\n",
    "# df_customers2 = df_customers.join(df_geolocation2, on=\"customer_id\", how=\"left\").select(\"customer_id\", \"birthdate\",\"first_join_date\",\"geolocation\",\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.5 Join data frames to find out the number of purchases the customer has made, add a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.6 Attach the transaction labels for fraud/non-fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring the Data <a class=\"anchor\" name=\"1.3\"></a>\n",
    "**1.3.1 With the feature_df, write code to show the basic statistics: a) For each numeric column, show count, mean, stddev, min, max, 25 percentile, 50 percentile, 75 percentile; b) For each non-numeric column, display the top-5 values and the corresponding counts; c) For each boolean column, display the value and count. (3%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_9705/4098316154.py\", line 11, in find_address\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/geocoders/nominatim.py\", line 372, in reverse\n    return self._call_geocoder(url, callback, timeout=timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/geocoders/base.py\", line 391, in _call_geocoder\n    raise\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/adapters.py\", line 472, in get_json\n    resp = self._request(url, timeout=timeout, headers=headers)\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/adapters.py\", line 494, in _request\n    raise GeocoderUnavailable(message)\ngeopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /reverse?lat=-6.12349294556289&lon=106.752524532902&format=json&addressdetails=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f1a1c5a8f40>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m25\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m75\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_9705/4098316154.py\", line 11, in find_address\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/geocoders/nominatim.py\", line 372, in reverse\n    return self._call_geocoder(url, callback, timeout=timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/geocoders/base.py\", line 391, in _call_geocoder\n    raise\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/adapters.py\", line 472, in get_json\n    resp = self._request(url, timeout=timeout, headers=headers)\n  File \"/opt/conda/lib/python3.10/site-packages/geopy/adapters.py\", line 494, in _request\n    raise GeocoderUnavailable(message)\ngeopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /reverse?lat=-6.12349294556289&lon=106.752524532902&format=json&addressdetails=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f1a1c5a8f40>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n"
     ]
    }
   ],
   "source": [
    "feature_df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2 Explore the dataframe and write code to present two plots worthy of presentation to the company, describe your plots and discuss the findings from the plots. (8%)**\n",
    "One of the plots needs to be based on feature_df in regard to fraudulent behaviour; you’re free to choose the other one.  \n",
    "Hint 1: You can use basic plots (e.g., histograms, line charts, scatter plots) to show the relationship between a column and the label or more advanced plots like correlation plots.  \n",
    "Hint 2: If your data is too large for plotting, consider using sampling before plotting.  \n",
    "150 words max for each plot’s description and discussion  \n",
    "Feel free to use any plotting libraries: matplotlib, seabon, plotly, etc.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Feature extraction and ML training <a class=\"anchor\" name=\"part-2\"></a>\n",
    "In this section, you must use PySpark DataFrame functions and ML packages for data preparation, model building, and evaluation. Other ML packages, such as scikit-learn, would receive zero marks.\n",
    "### 2.1 Discuss the feature selection and prepare the feature columns\n",
    "\n",
    "2.1.1 Based on the data exploration from 1.2 and considering the use case, discuss the importance of those features (For example, which features may be useless and should be removed, which feature has a significant impact on the label column, which should be transformed), which features you are planning to use? Discuss the reasons for selecting them and how you create/transform them\n",
    "300 words max for the discussion\n",
    "Please only use the provided data for model building\n",
    "You can create/add additional features based on the dataset\n",
    "Hint - Use the insights from the data exploration/domain knowledge/statistical models to consider whether to create more feature columns, whether to remove some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Write code to create/transform the columns based on your discussion above\n",
    "Hint: You can use one data frame for both use cases (classification and k-mean later in part 3) since you can select your desired columns as the input and output for each use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing Spark ML Transformers/Estimators for features, labels, and models  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "\n",
    "**2.2.1 Write code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1 and create ML model Estimators for Random Forest (RF) and Gradient-boosted tree (GBT) model.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2. Write code to include the above Transformers/Estimators into two pipelines.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the training data and testing data  \n",
    "Write code to split the data for training and testing purposes.\n",
    "Note: Due to the large dataset size, you can use random sampling (say 20% of the dataset) and do a train/test split or use one year of data for training and another year for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and evaluating models  \n",
    "2.4.1 Write code to use the corresponding ML Pipelines to train the models on the training data from 2.3. And then use the trained models to predict the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 For both models (RF and GBT) and testing data, write code to display the count of TP/TN/FP/FN. Compute the AUC, accuracy, recall, and precision for the above-threshold/below-threshold label from each model testing result using PySpark MLlib/ML APIs.\n",
    "Draw a ROC plot.\n",
    "Discuss which one is the better model (no word limit; please keep it concise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 Save the better model (you need it for Part B of Assignment 2).\n",
    "(Note: You may need to go through a few training loops or use more data to create a better-performing model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Customer Clustering and Knowledge sharing with K-Mean <a class=\"anchor\" name=\"part-3\"></a>  \n",
    "Please see the specification for this task and add code/markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Data Ethics, Privacy, and Security <a class=\"anchor\" name=\"part-4\"></a>  \n",
    "Please see the specification for this task and add markdown cells(word limit: 500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "Please add your references below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
