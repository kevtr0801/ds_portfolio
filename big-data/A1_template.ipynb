{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 2024 S2 Assignment 1 : Analysing Fraudulent Transaction Data\n",
    "\n",
    "## Table of Contents\n",
    "* [Part 1 : Working with RDD](#part-1)  \n",
    "    - [1.1 Data Preparation and Loading](#1.1)  \n",
    "    - [1.2 Data Partitioning in RDD](#1.2)  \n",
    "    - [1.3 Query/Analysis](#1.3)  \n",
    "* [Part 2 : Working with DataFrames](#2-dataframes)  \n",
    "    - [2.1 Data Preparation and Loading](#2-dataframes)  \n",
    "    - [2.2 Query/Analysis](#2.2)  \n",
    "* [Part 3 :  RDDs vs DataFrame vs Spark SQL](#part-3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Working with RDDs (30%) <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Working with RDD\n",
    "In this section, you will need to create RDDs from the given datasets, perform partitioning in these RDDs and use various RDD operations to answer the queries. \n",
    "\n",
    "1.1.1 Data Preparation and Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "Write the code to create a SparkContext object using SparkSession. To create a SparkSession you first need to build a SparkConf object that contains information about your application, use Melbourne time as the session timezone. Give an appropriate name for your application and run Spark locally with 4 cores on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "# Specify the master and application name\n",
    "master = \"local[4]\"  \n",
    "app_name = \"FIT5202 A1\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sql(\"SET TIME ZONE 'Australia/Melbourne'\")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Load csv files into multiple RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_rdd  = sc.textFile('category.csv')\n",
    "customers_rdd = sc.textFile('customers.csv')\n",
    "geolocation_rdd = sc.textFile('geolocation.csv')\n",
    "merchant_rdd = sc.textFile('merchant.csv')\n",
    "transactions_rdd = sc.textFile('transactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 For each RDD, remove the header rows and display the total count and first 10 records. (Hint: You can use csv.reader to parse rows into RDDs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "def remove_header(rdd):\n",
    "    rdd = rdd.map(lambda line: line.split(','))\n",
    "    header = rdd.first()\n",
    "    rdd = rdd.filter(lambda row: row!=header)\n",
    "    return rdd\n",
    "    \n",
    "category_rdd = remove_header(category_rdd)\n",
    "customers_rdd = remove_header(customers_rdd)\n",
    "geolocation_rdd = remove_header(geolocation_rdd)\n",
    "merchant_rdd = remove_header(merchant_rdd)\n",
    "transactions_rdd = remove_header(transactions_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Entertainment', '1'],\n",
       " ['Food_Dining', '2'],\n",
       " ['Gas_Transport', '3'],\n",
       " ['Grocery(Online)', '4'],\n",
       " ['Grocery(In Store)', '5'],\n",
       " ['Health_Fitness', '6'],\n",
       " ['Home', '7'],\n",
       " ['Pets', '8'],\n",
       " ['Misc(Online)\\\\', '9'],\n",
       " ['Misc(In Store)', '10']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"263-99-6044\"',\n",
       "  '\"4241904966319315\"',\n",
       "  'Melissa',\n",
       "  'Turner',\n",
       "  'F',\n",
       "  '\"058 Stanley Cliff\"',\n",
       "  'Risk manager',\n",
       "  '\"2005-05-30\"',\n",
       "  '376443331852',\n",
       "  '6339'],\n",
       " ['\"292-61-7844\"',\n",
       "  '\"30520471167198\"',\n",
       "  'Mark',\n",
       "  'Brown',\n",
       "  'M',\n",
       "  '\"413 Angela Mall\"',\n",
       "  'Trading standards officer',\n",
       "  '\"2003-04-19\"',\n",
       "  '870143739098',\n",
       "  '6200'],\n",
       " ['\"491-28-3311\"',\n",
       "  '\"180084219933088\"',\n",
       "  'Courtney',\n",
       "  'Hall',\n",
       "  'F',\n",
       "  '\"5712 Tamara Estate\"',\n",
       "  'Optometrist',\n",
       "  '\"2002-04-17\"',\n",
       "  '965855026307',\n",
       "  '3547'],\n",
       " ['\"826-23-1754\"',\n",
       "  '\"2623398454615676\"',\n",
       "  'Krystal',\n",
       "  'Branch',\n",
       "  'F',\n",
       "  '\"1016 Bennett Mountains\"',\n",
       "  'Banker',\n",
       "  '\"2001-07-15\"',\n",
       "  '11324746755',\n",
       "  '6302'],\n",
       " ['\"172-11-9264\"',\n",
       "  '\"639034043849\"',\n",
       "  'Carol',\n",
       "  'Ellis',\n",
       "  'F',\n",
       "  '\"819 Joseph Plains Suite 807\"',\n",
       "  'Sports coach',\n",
       "  '\"2003-11-21\"',\n",
       "  '113495175185',\n",
       "  '5227'],\n",
       " ['\"150-95-7922\"',\n",
       "  '\"343731453038560\"',\n",
       "  'Julie',\n",
       "  'Gibson',\n",
       "  'F',\n",
       "  '\"51844 Nicholas Lane\"',\n",
       "  'Medical secretary',\n",
       "  '\"2006-03-06\"',\n",
       "  '719783599768',\n",
       "  '4047'],\n",
       " ['\"841-99-2980\"',\n",
       "  '\"3525799136621031\"',\n",
       "  'Joseph',\n",
       "  'Blankenship',\n",
       "  'M',\n",
       "  '\"91279 Natalie Place Apt. 172\"',\n",
       "  'Toxicologist',\n",
       "  '\"2005-07-01\"',\n",
       "  '908554315130',\n",
       "  '6271'],\n",
       " ['\"705-41-6699\"',\n",
       "  '\"342694486959460\"',\n",
       "  'Nicole',\n",
       "  'Gutierrez',\n",
       "  'F',\n",
       "  '\"58874 Lane Trail Suite 213\"',\n",
       "  'Product manager',\n",
       "  '\"2003-01-23\"',\n",
       "  '772162574642',\n",
       "  '6302'],\n",
       " ['\"016-22-4524\"',\n",
       "  '\"3563009792513271\"',\n",
       "  'Anna',\n",
       "  'Montgomery',\n",
       "  'F',\n",
       "  '\"52812 Hall Point\"',\n",
       "  '\"Loss adjuster',\n",
       "  ' chartered\"',\n",
       "  '\"2001-08-26\"',\n",
       "  '982712248618',\n",
       "  '5614'],\n",
       " ['\"639-46-2126\"',\n",
       "  '\"3587729343010715\"',\n",
       "  'Nancy',\n",
       "  'Clark',\n",
       "  'F',\n",
       "  '\"0558 Alex Flats Suite 414\"',\n",
       "  'Hydrologist',\n",
       "  '\"2005-02-10\"',\n",
       "  '603471636817',\n",
       "  '6328']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Burkeville', 'TX', '75932', '31.0099', '-93.6585', '1', '1437'],\n",
       " ['Fresno', 'TX', '77545', '29.5293', '-95.4626', '2', '19431'],\n",
       " ['Osseo', 'MN', '55311', '45.1243', '-93.4996', '3', '65312'],\n",
       " ['Pomona', 'CA', '91766', '34.0418', '-117.7569', '4', '154204'],\n",
       " ['Vacaville', 'CA', '95688', '38.3847', '-121.9887', '5', '99475'],\n",
       " ['South Lake Tahoe', 'CA', '96150', '38.917', '-119.9865', '6', '29800'],\n",
       " ['Belvidere', 'TN', '37306', '35.1415', '-86.1728', '7', '2760'],\n",
       " ['Columbia', 'SC', '29205', '33.9903', '-80.9997', '8', '333497'],\n",
       " ['Chicago', 'IL', '60660', '41.9909', '-87.6629', '9', '2680484'],\n",
       " ['Tunnelton', 'WV', '26444', '39.3625', '-79.7478', '10', '3639']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geolocation_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bins-Tillman', '6051', '1'],\n",
       " ['\"Hahn', ' Douglas and Schowalter\"', '1276', '2'],\n",
       " ['\"Hayes', ' Marquardt and Dibbert\"', '1383', '3'],\n",
       " ['\"Mueller', ' Gerhold and Mueller\"', '1846', '4'],\n",
       " ['Kerluke Inc', '1784', '5'],\n",
       " ['Waelchi Inc', '4637', '6'],\n",
       " ['Trantow PLC', '2176', '7'],\n",
       " ['Runolfsson and Sons', '3968', '8'],\n",
       " ['Bechtelar-Rippin', '1048', '9'],\n",
       " ['\"Schumm', ' Bauch and Ondricka\"', '1553', '10']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"0c20530e90719213c442744161a1850b\"',\n",
       "  '1622367050',\n",
       "  '87.18',\n",
       "  '0',\n",
       "  '\"794-45-4364\"',\n",
       "  '46',\n",
       "  '2641132',\n",
       "  '12'],\n",
       " ['\"984fc48fc946605deefc9d0967582811\"',\n",
       "  '1609183538',\n",
       "  '276.97',\n",
       "  '0',\n",
       "  '\"436-80-2340\"',\n",
       "  '60',\n",
       "  '2932280',\n",
       "  '5'],\n",
       " ['b13ff47c73689bc4c8320c0ce403b15d',\n",
       "  '1655595319',\n",
       "  '7.67',\n",
       "  '0',\n",
       "  '\"385-77-6544\"',\n",
       "  '87',\n",
       "  '2708770',\n",
       "  '2'],\n",
       " ['\"7cffae35cab67d9415f9f22d91ca7acc\"',\n",
       "  '1613234460',\n",
       "  '198.96',\n",
       "  '0',\n",
       "  '\"450-56-1117\"',\n",
       "  '138',\n",
       "  '1170872',\n",
       "  '10'],\n",
       " ['\"22e01cb3403a4c7ce598ebe785e1e947\"',\n",
       "  '1605030979',\n",
       "  '33.46',\n",
       "  '0',\n",
       "  '\"397-54-0253\"',\n",
       "  '218',\n",
       "  '2470519',\n",
       "  '5'],\n",
       " ['\"1d174d018228efcd1d5800f768628904\"',\n",
       "  '1608989049',\n",
       "  '2.74',\n",
       "  '0',\n",
       "  '\"248-09-7729\"',\n",
       "  '222',\n",
       "  '3436926',\n",
       "  '9'],\n",
       " ['\"532536d65907e08d938cb31e3631ddd4\"',\n",
       "  '1650997797',\n",
       "  '1.23',\n",
       "  '0',\n",
       "  '\"277-12-7638\"',\n",
       "  '337',\n",
       "  '3750746',\n",
       "  '2'],\n",
       " ['\"32d76f65b7512afbdc99331ee96bc6d7\"',\n",
       "  '1649986601',\n",
       "  '7.78',\n",
       "  '0',\n",
       "  '\"615-63-3623\"',\n",
       "  '718',\n",
       "  '3773961',\n",
       "  '2'],\n",
       " ['c3f29bca602c9e2e9a188567f06d632f',\n",
       "  '1617032215',\n",
       "  '218.8',\n",
       "  '0',\n",
       "  '\"877-16-8226\"',\n",
       "  '747',\n",
       "  '2377216',\n",
       "  '10'],\n",
       " ['c56ef2e4a43d867128839b97bc1dbb66',\n",
       "  '1609250028',\n",
       "  '62.1',\n",
       "  '0',\n",
       "  '\"823-85-5801\"',\n",
       "  '950',\n",
       "  '652447',\n",
       "  '5']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.4 Drop personal information columns from RDDs: cc_num, firstname, lastname, address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Entertainment', '1'],\n",
       " ['Food_Dining', '2'],\n",
       " ['Gas_Transport', '3'],\n",
       " ['Grocery(Online)', '4'],\n",
       " ['Grocery(In Store)', '5'],\n",
       " ['Health_Fitness', '6'],\n",
       " ['Home', '7'],\n",
       " ['Pets', '8'],\n",
       " ['Misc(Online)\\\\', '9'],\n",
       " ['Misc(In Store)', '10']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_columns(rdd):\n",
    "    if ['cc_num', 'firstname', 'lastname', 'address'] in rdd:\n",
    "        rdd = rdd.drop('cc_num', 'firstname', 'lastname', 'address')\n",
    "        return rdd\n",
    "    else:\n",
    "        None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Partitioning in RDD <a class=\"anchor\" name=\"1.2\"></a>\n",
    "1.2.1 For each RDD, print out the total number of partitions and the number of records in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 Answer the following questions:   \n",
    "a) How many partitions do the above RDDs have?   \n",
    "b) How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy? Can you explain why it is partitioned in this number?   \n",
    "c) Assuming we are querying the dataset based on transaction date, can you think of a better strategy to partition the data based on your available hardware resources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer for a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer for b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer for c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3 Create a user defined function (UDF) to transform trans_timestamp to ISO format(YYYY-MM-DD hh:mm:ss), then call the UDF and add a new column trans_datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query/Analysis <a class=\"anchor\" name=\"1.3\"></a>\n",
    "For this part, write relevant RDD operations to answer the following queries.\n",
    "\n",
    "1.3.1 Calculate the summary of fraudulent transactions amount for each year, each month. Print the results in tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 List 20 mechants that suffered the most from fraudulent activities(i.e. 20 highest amount of monetary loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Working with DataFrames (45%) <a class=\"anchor\" name=\"2-dataframes\"></a>\n",
    "In this section, you need to load the given datasets into PySpark DataFrames and use DataFrame functions to answer the queries.\n",
    "### 2.1 Data Preparation and Loading\n",
    "\n",
    "2.1.1. Load the CSV files into separate dataframes. When you create your dataframes, please refer to the metadata file and think about the appropriate data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41278)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_category = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('category.csv')\n",
    "\n",
    "df_customers = spark.read.format('csv')\\\n",
    "            .option('header',True).option('esacpe','\"')\\\n",
    "            .load('customers.csv')\n",
    "\n",
    "df_geolocation = spark.read.format('csv')\\\n",
    "            .option('header',True).option('esacpe','\"')\\\n",
    "            .load('geolocation.csv')\n",
    "\n",
    "df_merchant = spark.read.format('csv')\\\n",
    "            .option('header',True).option('esacpe','\"')\\\n",
    "            .load('merchant.csv')\n",
    "\n",
    "df_transactions = spark.read.format('csv')\\\n",
    "            .option('header',True).option('esacpe','\"')\\\n",
    "            .load('transactions.csv')\n",
    "\n",
    "df_category.createOrReplaceTempView(\"sql_category\")\n",
    "df_customers.createOrReplaceTempView(\"sql_customers\")\n",
    "df_geolocation.createOrReplaceTempView(\"sql_geolocation\")\n",
    "df_merchant.createOrReplaceTempView(\"sql_merchant\")\n",
    "df_transactions.createOrReplaceTempView(\"sql_transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Display the schema of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id_category: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id_customer: string (nullable = true)\n",
      " |-- cc_num: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- acct_num: string (nullable = true)\n",
      " |-- id_geolocation: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- id_geolocation: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- id_geolocation: string (nullable = true)\n",
      " |-- id_merchant: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id_transaction: string (nullable = true)\n",
      " |-- trans_timestamp: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      " |-- is_fraud: string (nullable = true)\n",
      " |-- id_customer: string (nullable = true)\n",
      " |-- id_geolocation: string (nullable = true)\n",
      " |-- id_merchant: string (nullable = true)\n",
      " |-- id_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_category.printSchema()\n",
    "df_customers.printSchema()\n",
    "df_geolocation.printSchema()\n",
    "df_merchant.printSchema()\n",
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about: When the dataset is large, do you need all columns? How to optimize memory usage? Do you need a customized data partitioning strategy? (note: You don’t need to answer these questions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 QueryAnalysis  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "Implement the following queries using dataframes. You need to be able to perform operations like filtering, sorting, joining and group by using the functions provided by the DataFrame API.   \n",
    "\n",
    "2.2.1. Transform the “trans_timestamp” to multiple columns: trans_year, trans_month, trans_day, trans_hour(24-hour format). (note: you can reuse your UDF from part 1 or create a new one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2. Calculate the total amount of fraudulent transactions for each hour. Show the result in a table and plot a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 Print number of small transactions(<=$100) from female who was born after 1990. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.4 We consider a fraud-to-sales(F2S) ratio of 3% as a benchmark. If a merchant has F2S >= 3%, it is considered operating at very high rick. How many companies are operating at very high risk? (note: The answer should be a single number.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.5 “Abbott and Adam Group” wants to know their total revenue(sum of non-fraud amt) in each state they operate, show the top 20 results by revenue in descending order. You output should include merchant name, state and total revenue. (note: Abbott and Adam group include all merchants who name start with “Abbott” or “Adam”.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.6 For each year (2020-2022), aggregate the number(count) of fraudulent transactions every hour. Plot an appropriate figure and observe the trend. Write your observations from your plot (e.g. Is fraudulent activities increasing or decreasing? Are those frauds more active after midnight or during business hours?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 RDDs vs DataFrame vs Spark SQL (25%) <a class=\"anchor\" name=\"part-3\"></a>\n",
    "Implement the following queries using RDDs, DataFrame in SparkSQL separately. Log the  time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference between these 3 approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query: <strong>We consider city with population < 50K as small(denoted as S); 50K-200K as medium(M), >200K as large(L). For each city type, using customer age bucket of 10(e.g. 0-9, 10-19, 20-29…), show the percentage ratio of fraudulent transactions in each age bucket.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. RDD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. DataFrame Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Spark SQL Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Which one is the easiest to implement in your opinion? Log the time taken for each query, and observe the query execution time, among RDD, DataFrame, SparkSQL, which is the fastest and why? Please include proper reference. (Maximum 500 words.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some ideas on the comparison\n",
    "\n",
    "Armbrust, M., Huai, Y., Liang, C., Xin, R., & Zaharia, M. (2015). Deep Dive into Spark SQL’s Catalyst Optimizer. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "\n",
    "Damji, J. (2016). A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets. Retrieved September 28, 2017, from https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "Data Flair (2017a). Apache Spark RDD vs DataFrame vs DataSet. Retrieved September 28, 2017, from http://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset\n",
    "\n",
    "Prakash, C. (2016). Apache Spark: RDD vs Dataframe vs Dataset. Retrieved September 28, 2017, from http://why-not-learn-something.blogspot.com.au/2016/07/apache-spark-rdd-vs-dataframe-vs-dataset.html\n",
    "\n",
    "Xin, R., & Rosen, J. (2015). Project Tungsten: Bringing Apache Spark Closer to Bare Metal. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
